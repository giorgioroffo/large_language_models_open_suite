# Define some variables - model names
model_name: "google/gemma-2b"
new_model: "gemma-ft"

################################################################################
# LoRA parameters
################################################################################
lora_parameters:
  lora_r: 4  # LoRA attention dimension
  lora_alpha: 16  # Alpha parameter for LoRA scaling
  lora_dropout: 0.1  # Dropout probability for LoRA layers

################################################################################
# bitsandbytes parameters
################################################################################
bitsandbytes_parameters:
  use_4bit: true  # Activate 4-bit precision base model loading
  bnb_4bit_compute_dtype: "float16"  # Compute dtype for 4-bit base models
  bnb_4bit_quant_type: "nf4"  # Quantization type (fp4 or nf4)
  use_nested_quant: false  # Activate nested quantization for 4-bit base models (double quantization)

################################################################################
# TrainingArguments parameters
################################################################################
training_arguments:
  output_dir: "./results"  # Output directory where the model predictions and checkpoints will be stored
  num_train_epochs: 1  # Number of training epochs
  fp16: false  # Enable fp16 training
  bf16: false  # Enable bf16 training
  per_device_train_batch_size: 4  # Batch size per GPU for training
  per_device_eval_batch_size: 4  # Batch size per GPU for evaluation
  gradient_accumulation_steps: 1  # Number of update steps to accumulate the gradients for
  gradient_checkpointing: true  # Enable gradient checkpointing
  max_grad_norm: 0.3  # Maximum gradient normal (gradient clipping)
  learning_rate: 0.0002  # Initial learning rate (AdamW optimizer)
  weight_decay: 0.001  # Weight decay to apply to all layers except bias/LayerNorm weights
  optim: "paged_adamw_32bit"  # Optimizer to use
  lr_scheduler_type: "constant"  # Learning rate schedule (constant a bit better than cosine)
  max_steps: -1  # Number of training steps (overrides num_train_epochs)
  warmup_ratio: 0.03  # Ratio of steps for a linear warmup (from 0 to learning rate)
  group_by_length: true  # Group sequences into batches with same length
  save_steps: 25  # Save checkpoint every X updates steps
  logging_steps: 25  # Log every X updates steps

################################################################################
# SFT parameters
################################################################################
sft_parameters:
  max_seq_length: 40  # Maximum sequence length to use
  packing: true  # Pack multiple short examples in the same input sequence to increase efficiency
  device_map: "auto"  # Load the entire model on the GPU 0